# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
otelcol-logs:
  enabled: true
  # git clone https://github.com/open-telemetry/opentelemetry-helm-charts
  # opentelemetry-helm-charts/charts/opentelemetry-collector/values.yaml
  mode: daemonset
  image:
    # Check manifests.yaml for different components available:
    # https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions
    # If you want a lightweight, production-ready OTEL Collector specifically for K8s observability (metrics/logs/traces)
    # repository: "otel/opentelemetry-collector-k8s"
    # If you need access to less common or experimental components, or broader telemetry sources
    repository: "otel/opentelemetry-collector-contrib"

  # to access pod otelcol by node ip (only for debug)
  # BREAKS access to internal cluster kubernetes network
  # no access to e.g. loki-gateway.monitoring.svc.cluster.local
  #hostNetwork: true

  # only works for "deployment" mode !!!
  #serviceMonitor:
  #  enabled: true
  #  extraLabels:
  #    release: telemetry

  # only works for "daemonset" mode !!!
  # You'd enable a PodMonitor in OTEL Collector only if you want Prometheus to scrape metrics from the collector (e.g., metrics about the collector itself like pipeline throughput, dropped data, etc.)
  podMonitor:
    enabled: true
    extraLabels: # TODO to be deleted when included inside single rag-telemetry helm-chart
      release: observability

  # this enable filelog receiver configured for k8s, https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector#configuration-for-kubernetes-container-logs
  # it deploys a DaemonSet collector to scrape logs from /var/log/pods/**/*.log
  presets:
    logsCollection:
      enabled: true
      #enabled: false # disable for debugging
      includeCollectorLogs: false # default: false, do not include internal logs, to prvent logs explosion https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector#warning-warning-risk-of-looping-the-exported-logs-back-into-the-receiver-causing-log-explosion
    kubernetesAttributes:
      enabled: true

  resources:
    limits:
      memory: 200Mi

  service:
    type: ClusterIP

  # If OTLP ports are disabled, the collector can still ingest data from other sources like file logs, Prometheus targets, host metrics, and more — as long as you configure those receivers.
  # You only need OTLP if you're accepting telemetry from external SDKs or agents.
  ports:
    otlp:
      enabled: false
    otlp-http:
      enabled: false
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    # prometheus:
    #   enabled: true
    #   containerPort: 9464
    #   servicePort: 9464
    #   protocol: TCP


  # Use alternateConfig to be able to null pipelines/receivers: please see check this for details
  # https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/values.yaml#L187
  # config:
  alternateConfig:
    # if you're using alternateConfig (a full override), you must explicitly define everything, including the filelog receiver, because presets are ignored in this case.
    receivers:
      # filelog: {} 
      filelog:
        include:
          - /var/log/pods/**/*.log
          # - /var/log/containers/*.log 
        start_at: beginning
        include_file_name: true
        include_file_path: true
    exporters:
      ## Loki native OTLP protocol
      # https://grafana.com/docs/loki/latest/send-data/otel/
      # https://grafana.com/docs/grafana-cloud/send-data/otlp/send-data-otlp/
      # https://grafana.com/docs/loki/latest/send-data/otel/native_otlp_vs_loki_exporter/
      # Requires: limits_config: allow_structured_metadata: true in Loki configuration
      # limits_config:
      #   allow_structured_metadata: true
      otlphttp/loki:
        ### monolithic
        # with gateway: dont use without "v1/logs" just otlp and dont pass port) - gateway is on 80
        #endpoint: http://loki-gateway.observability.svc.cluster.local/otlp/
        # with direct connection (specify both port)
        #endpoint: http://loki.observability.svc.cluster.local:3100/otlp/v1/logs
        ### simplescalale using service loki-write
        endpoint: http://loki-write.observability.svc.cluster.local:3100/otlp/
        auth:
          authenticator: headers_setter/ocd

      ## Debug exporter
      debug:
        #use_internal_logger: true
        #verbosity: basic
        #verbosity: normal
        verbosity: detailed

    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
      headers_setter/ocd:
        headers:
          - action: insert
            key: X-Scope-OrgID # Multi-tenancy in Loki means multiple users/teams share the same Loki backend, but each one’s logs are separated and isolated using a tenant ID (X-Scope-OrgID). OTEL helps by injecting that tenant ID automatically using the headers_setter extension.
            #from_context: tenant_id
            value: ent-inference-loki

    service:
      extensions:
        - health_check
        - headers_setter/ocd
      telemetry:
        logs:
          # the same can be achieved by adding pod.container.args: --set=service::telemetry::logs::level=debug
          # level: info
          level: debug
      pipelines:
        logs:
          receivers:
          - filelog
          exporters:
          - otlphttp/loki
          - debug

### =================================================
### LOKI subchart configuraion
### =================================================
loki:
  enabled: true
  # DNS service need to match deployed kubernetes
  # kubectl get svc --namespace=kube-system -l k8s-app=kube-dns  -o jsonpath='{.items..metadata.name}'
  # or disable with extra helm argument: "--set loki.global.dnsService=null" when install telemetry-logs
  global:
    # CoreDNS is the default DNS server in Kubernetes clusters (since Kubernetes v1.13+). It handles all internal DNS resolution
    dnsService: "coredns"     # for kubespray based setup

  loki:
    commonConfig:
      # Version for single node deployment (required for single node setups).
      # **Warning** this disables HA for write path.
      # Note: This need be with sync with number of ingesters (write.replicas)
      replication_factor: 1

    schemaConfig:
      configs:
        - from: 2024-04-01
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: dnvrdev/loki_index_
            period: 24h
    storage_config:
      aws:
        region: us-west-2
        bucketnames: 
        access_key_id: 
        secret_access_key: 
        endpoint: s3.amazonaws.com
        s3forcepathstyle: false
    storage:
      bucketNames:
        chunks: 
        ruler: 
        admin: 
    compactor:
      working_directory: /tmp/retention
      compaction_interval: 10m
      retention_enabled: true
      retention_delete_delay: 2h
      retention_delete_worker_count: 150
      delete_request_store: s3
    limits_config:
      retention_period: 168h
    # Determines how log chunks are compressed in memory and storage. Snappy, A fast compression algorithm optimized for speed over maximum compression.
    # Alternatives: gzip, lz4, zstd, or none (not recommended in production).
    ingester:
      chunk_encoding: snappy
    querier:
      # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
      max_concurrent: 4
    # https://grafana.com/docs/loki/latest/send-data/otel/#format-considerations
    distributor:
      otlp_config:
        default_resource_attributes_as_index_labels:
        - "k8s.node.name" # extra attribute thanks to resourcedetection/k8snode (check values-journalctl.yaml)
        - "cloud.availability_zone"
        - "cloud.region"
        - "container.name"
        - "deployment.environment"
        - "k8s.cluster.name"
        - "k8s.container.name"
        - "k8s.cronjob.name"
        - "k8s.daemonset.name"
        - "k8s.deployment.name"
        - "k8s.job.name"
        - "k8s.namespace.name"
        - "k8s.pod.name"
        - "k8s.replicaset.name"
        - "k8s.statefulset.name"
        - "service.instance.id"
        - "service.name"
        - "service.namespace"

  deploymentMode: SimpleScalable

  # keep those values in-sync with commonConfig.replication_factor
  backend:
    replicas: 1
  read:
    replicas: 1
  write:
    replicas: 1
  # Enable minio for storage
  minio:
    enabled: false

  ### SELF-LOGGING
  # Warning: THIS Is depreacted will be removed in future versions
  monitoring:
    dashboards:
      enabled: true
      namespace: observability
    rules:
      enabled: true
      labels:
        release: observability
    serviceMonitor:
      enabled: true
      labels:
        release: observability

  gateway:
    # -- Specifies whether the gateway should be enabled
    enabled: false