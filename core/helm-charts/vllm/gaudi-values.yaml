# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# User-configurable parameters (can be set via --set during helm install)
accelDevice: "gaudi"

block_size: 128          # Default KV cache block size
max_num_seqs: 256        # Default maximum number of sequences
max_seq_len_to_capture: 2048 # Default maximum sequence length
d_type: "bfloat16"
max_model_len: 5120

image:
  repository: opea/vllm-gaudi
  tag: "1.3.1"
  pullPolicy: IfNotPresent


affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: node-role.kubernetes.io/inference-gaudi
              operator: In
              values:
                - "true"


runtime: "habana"
HABANA_VISIBLE_DEVICES: "all"
VLLM_NO_USAGE_STATS: 1
DO_NOT_TRACK: 1



LLM_MODEL_ID: ""

modelConfigs:        
  # meta-llama/Llama-3.1-8B-Instruct
  "meta-llama/Llama-3.1-8B-Instruct":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1.0
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8128
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.09
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 32512
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","32512","--gpu-memory-util","0.98","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"
  
  # meta-llama/Llama-3.1-70B-Instruct
  "meta-llama/Llama-3.1-70B-Instruct":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8256
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.13
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"

  # meta-llama/Llama-3.3-70B-Instruct
  "meta-llama/Llama-3.3-70B-Instruct":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8256
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.05
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"

  # mistralai/Mistral-7B-Instruct-v0.2
  "mistralai/Mistral-7B-Instruct-v0.2":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8192
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.09
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 32768
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","32768","--gpu-memory-util","0.98","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests"]
    tensor_parallel_size: "1"
  
  # mistralai/Mistral-7B-Instruct-v0.1
  "mistralai/Mistral-7B-Instruct-v0.1":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8192
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.09
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 32768
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","32768","--gpu-memory-util","0.98","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests"]
    tensor_parallel_size: "1"
  
  # mistralai/Mixtral-8x7B-Instruct-v0.1
  "mistralai/Mixtral-8x7B-Instruct-v0.1":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1.0
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8128
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.17
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 32512
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","32512","--gpu-memory-util","0.97","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests"]
    tensor_parallel_size: "2"

  # tiiuae/Falcon3-7B-Instruct
  "tiiuae/Falcon3-7B-Instruct":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0 
      PT_HPU_LAZY_MODE: 1 
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1 
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8256 
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256 
      VLLM_DECODE_BS_BUCKET_STEP: 32 
      VLLM_DELAYED_SAMPLING: false 
      VLLM_GRAPH_PROMPT_RATIO: 0.8 
      VLLM_GRAPH_RESERVED_MEM: 0.09 
      VLLM_PROMPT_BS_BUCKET_STEP: 32 
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024 
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256 
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests"]
    tensor_parallel_size: "1"    

  # deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  "deepseek-ai/DeepSeek-R1-Distill-Llama-8B":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0 
      PT_HPU_LAZY_MODE: 1 
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1 
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8256 
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256 
      VLLM_DECODE_BS_BUCKET_STEP: 32 
      VLLM_DELAYED_SAMPLING: false 
      VLLM_GRAPH_PROMPT_RATIO: 0.8 
      VLLM_GRAPH_RESERVED_MEM: 0.09 
      VLLM_PROMPT_BS_BUCKET_STEP: 32 
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024 
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256 
      VLLM_SKIP_WARMUP: false 
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 8256
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.13
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"
  
# Default arguments if the model ID is not found
defaultModelConfigs:
  configMapValues:
    LLM_MODEL_ID: ""
    HF_HUB_DISABLE_XET: 1
    OMPI_MCA_btl_vader_single_copy_mechanism: none
    PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
    VLLM_CPU_KVCACHE_SPACE: "40"
  extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
  tensor_parallel_size: "1"