# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
---
- name: Deploy Inference LLM Models
  hosts: "{{ inference_delegate | default('kube_control_plane') }}"
  gather_facts: false
  any_errors_fatal: "{{ any_errors_fatal | default(true) }}"
  environment: "{{ proxy_disable_env | default(env_proxy | default({})) }}"
  vars_files:
    - "{{ lookup('env', 'PWD') }}/config/vault.yml"
    - "{{ lookup('env', 'PWD') }}/config/vars/inference_llm_models.yml"   
    - "{{ lookup('env', 'PWD') }}/config/inference_env.yml" 
  roles:
    - role: inference-tools
  tasks:  
    - name: Print active tags for this task
      debug:
        var: ansible_run_tags
      tags: always
      run_once: true
    - name: Setup Environment
      block:
        - name: Create/Update Kubernetes Secret for Hugging Face Token
          kubernetes.core.k8s:
            name: hugging-face-token
            namespace: default        
            kind: Secret
            definition:
              stringData:
                hugging-face-token: "{{ hugging_face_token }}"
            apply: yes
            state: present
          run_once: true
          when: hugging_face_token is defined    
          tags: always        
        - name: Ensure Remote Directory Exists
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}"
            state: directory
            mode: '0755'
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"    
          tags: always
        - name: Sync dependency files to Deployment Nodes
          ansible.posix.synchronize:
            src: "{{ item.src }}/"
            dest: "{{ item.dest }}/"
            recursive: yes
            delete: no
            mode: push
          loop:
            - { src: "{{ helm_charts_base }}", dest: "{{ remote_helm_charts_base }}/" }
          tags: always                    
        - name: Transfer Dependency keycloak-realmcreationfile
          ansible.builtin.copy:
            src: "{{ remote_home_dir }}/"
            dest: "{{ remote_helm_charts_base }}/"
            mode: '0755'
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            remote_src: no
          tags: install-keycloak-apisix
        - name: Ensure the script is executable
          file:
            path: "{{ remote_helm_charts_base }}/keycloak-realmcreation.sh"
            mode: '0755'
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
          tags: install-keycloak-apisix        
        - name: Update Dependencies for Auth-Apisix on Nodes
          ansible.builtin.command: helm dependency update "{{ remote_helm_charts_base }}/apisix-helm/"
          register: helm_dependency_update
          failed_when: helm_dependency_update.rc != 0
          when: apisix_enabled == "yes"
          tags: install-keycloak-apisix      
    - name: Fetch the keycloak client secret
      block:
        - name: Fetch the keycloak client secret
          command: "{{ remote_helm_charts_base }}/keycloak-realmcreation.sh {{ secret_name }} {{ keycloak_admin_user }} {{ keycloak_admin_password }} {{ keycloak_client_id }}"
          register: script_output
          environment:
            http_proxy: ""
            https_proxy: ""
            no_proxy: ""
        - name: Set Keycloak client fact
          set_fact:
            client_secret: "{{ script_output.stdout | regex_search('Client secret: (.*)') | join('') | regex_replace('^Client secret: ') }}"
          when: script_output.stdout is search('Client secret:')
        - name: Display Keycloak client
          debug:
            msg: "The client key is: {{ client_secret }}"        
      run_once: true
      when: deploy_keycloak == 'yes'
      tags: install-keycloak-apisix                            
    - name: env_proxy Contents
      debug:
        var: env_proxy
      tags: always    
    - name: Set proxy args if proxy is defined
      set_fact:
        helm_proxy_args: >-
          {% if env_proxy is defined %}
          --set global.http_proxy="{{ env_proxy.http_proxy | default('') | replace(',', '\\,') }}"
          --set global.https_proxy="{{ env_proxy.https_proxy | default('') | replace(',', '\\,') }}"
          --set global.no_proxy="{{ env_proxy.no_proxy | default('') | replace(',', '\\,') }}"
          {% else %}
          {{ '' }}
          {% endif %}
      tags: always        
    - name: Print model_name_list
      debug:
        var: model_name_list
      tags: always
      run_once: true
    - name: Deploy Llama8b LLM Model
      block:
        - name: Delete Ingress resource Llama8b from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-8b-ingress
            state: absent          
          tags: install-llama-8b
        - name: Delete Ingress resource Llama8b from auth-apisix namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-8b-ingress
            state: absent
          tags: install-llama-8b          
        - name: Deploy LLM model Llama8b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-8b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-8B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
              --values {{ gaudi_values_file }}              
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
          tags: install-llama-8b
        - name: Register Llama8b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-8B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-8B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-8b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-llama-8b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama-8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"  
      run_once: true
      when:                
        - model_name_list is defined        
        - "'llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - install_true == 'true'

    - name: Check if vllm-llama-8b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-8b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-8b
    - name: Uninstall vllm-llama-8b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-8b"
      run_once: true
      tags: uninstall-llama-8b
      when:
        - "'llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""
    

    - name: Deploy "{{ huggingface_model_id }}" LLM Model
      block:
        - name: Delete Ingress resource "{{ huggingface_model_id }}" from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: "{{huggingface_model_deployment_name}}-ingress"
            state: absent
          tags: "install-{{ huggingface_model_deployment_name }}"
        - name: Delete Ingress resource "{{ huggingface_model_id }}" from auth-apisix namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: "{{huggingface_model_deployment_name}}-ingress"
            state: absent
          tags: "install-{{ huggingface_model_deployment_name }}"
        - name: Deploy LLM model "{{ huggingface_model_id }}"
          ansible.builtin.command: >-
            helm upgrade --install "{{huggingface_model_deployment_name}}" "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="{{ huggingface_model_id }}"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment|lower == "true" %}
              --set tensor_parallel_size={{ huggingface_tensor_parellel_size }}
              --values {{ gaudi_values_file }}              
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register "{{ huggingface_model_id }}" model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "{{ huggingface_model_id }}"
            reg_litellm_model: "openai/{{ huggingface_model_id }}"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://{{huggingface_model_deployment_name}}-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - "install-{{ huggingface_model_deployment_name }}"
            - install-genai-gateway
          run_once: true
          when:
            - "'install-{{ huggingface_model_deployment_name }}' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: "install-{{ huggingface_model_deployment_name }}"
      when:                        
        - hugging_face_model_deployment == 'true'
        - install_true == 'true'                            
    - name: Check if "{{ hugging_face_model_remove_name }}" Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter {{ hugging_face_model_remove_name }} --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: "uninstall-{{ hugging_face_model_remove_name }}"
    - name: Uninstall "{{ hugging_face_model_remove_name }}" Model 
      ansible.builtin.command:
        cmd: "helm uninstall {{ hugging_face_model_remove_name }}"
      run_once: true
      tags: "uninstall-{{ hugging_face_model_remove_name }}"
      when:        
        - uninstall_true == 'true'
        - hugging_face_model_remove_deployment == 'true'
        - helm_release_installed.stdout != ""    
    - name: Deploy Llama70b LLM Model
      block:
        - name: Delete Ingress resource Llama70b from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-70b-ingress
            state: absent
          tags: install-llama-70b
        - name: Delete Ingress resource Llama70b from auth-apisix namespace
          tags: install-llama-70b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-70b-ingress
            state: absent
        - name: Deploy LLM model Llama70b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-70b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-70B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}            
            --set tensor_parallel_size=4
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0        
        - name: Register Llama70b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-70B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-70B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-70b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-llama-70b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama-70b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-llama-70b
      when:
        - '"llama-70b" in model_name_list'
        - install_true == 'true'

    - name: Check if vllm-llama-70b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-70b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-70b
    - name: Uninstall llama-70b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-70b"
      run_once: true
      tags: uninstall-llama-70b
      when:
        - "'llama-70b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Llama3-3-70b LLM Model
      block:
        - name: Delete Ingress resource Llama3.3-70b from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-3-3-70b-ingress
            state: absent          
          tags: install-llama-3-3-70b
        - name: Delete Ingress resource Llama3-3-70b from auth-apisix namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-3-3-70b-ingress
            state: absent
          tags: install-llama-3-3-70b          
        - name: Deploy LLM model Llama3-3-70b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-3-3-70b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.3-70B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size=4
              --values {{ gaudi_values_file }}              
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
          tags: install-llama-3-3-70b
      run_once: true
      when:
        - model_name_list is defined
        - "'llama-3-3-70b' in (model_name_list | regex_replace(',', ' ') | split())"
        - install_true == 'true'


    - name: Check if vllm-llama-3-3-70b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-3-3-70b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-3-3-70b
    - name: Uninstall vllm-llama-3-3-70b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-3-3-70b"
      run_once: true
      tags: uninstall-llama-3-3-70b
      when:
        - "'llama-3.3-70b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""


    - name: Deploy Mixtral7b LLM Model
      block:
        - name: Delete Ingress resource Mixtral7b from default namespace
          tags: install-mixtral-8x-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-mixtral-7b-ingress
            state: absent          
        - name: Delete Ingress resource Mixtral7b from auth-apisix namespace
          tags: install-mixtral-8x-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-mixtral-7b-ingress
            state: absent
        - name: Deploy LLM model Mixtral7b
          ansible.builtin.command: >-
            helm upgrade --install vllm-mixtral-7b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="mistralai/Mixtral-8x7B-Instruct-v0.1"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}            
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Mixtral7b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "mistralai/Mixtral-8x7B-Instruct-v0.1"
            reg_litellm_model: "openai/mistralai/Mixtral-8x7B-Instruct-v0.1"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-mixtral-7b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-mixtral-8x-7b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-mixtral-8x-7b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-mixtral-8x-7b
      when:
        - '"mixtral-8x-7b" in model_name_list'
        - install_true == 'true'

    - name: Check if Mixtral7b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-mixtral-7b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-mixtral-8x-7b
    - name: Uninstall mixtral-8x-7b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-mixtral-7b"
      run_once: true
      tags: uninstall-mixtral-8x-7b
      when:
        - "'mixtral-8x-7b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""


    - name: Deploy codellama LLM Model
      block:
        - name: Delete Ingress resource Codellama from default namespace
          tags: install-codellama-34b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-codellama-ingress
            state: absent
        - name: Delete Ingress resource Codellama from auth-apisix namespace
          tags: install-codellama-34b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-codellama-ingress
            state: absent
        - name: Deploy LLM model codellama
          ansible.builtin.command: >-
            helm upgrade --install vllm-codellama "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="codellama/CodeLlama-34b-Instruct-hf"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}            
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Codellama model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "codellama/CodeLlama-34b-Instruct-hf"
            reg_litellm_model: "openai/codellama/CodeLlama-34b-Instruct-hf"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-codellama-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-codellama-34b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-codellama-34b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      tags: install-codellama-34b
      when:
        - '"codellama-34b" in model_name_list'
        - install_true == 'true'
    - name: Check if codellama-34b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-codellama --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-codellama-34b
    - name: Uninstall codellama-34b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-codellama"
      run_once: true
      tags: uninstall-codellama-34b
      when:
        - "'codellama-34b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Faclon3 7b LLM Model
      block:
        - name: Delete Ingress resource Falcon3 7b from default namespace
          tags: install-falcon3-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-falcon3-7b-ingress
            state: absent
        - name: Delete Ingress resource Falcon3 7b from auth-apisix namespace
          tags: install-falcon3-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-falcon3-7b-ingress
            state: absent
        - name: Deploy LLM model Falcon3-7b
          ansible.builtin.command: >-
            helm upgrade --install vllm-falcon3-7b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="tiiuae/Falcon3-7B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}            
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Falcon3-7b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "tiiuae/Falcon3-7B-Instruct"
            reg_litellm_model: "openai/tiiuae/Falcon3-7B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-falcon3-7b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-falcon3-7b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-falcon3-7b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-falcon3-7b
      when:
        - '"falcon3-7b" in model_name_list'
        - install_true == 'true'
    - name: Check if Falcon3 7b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-falcon3-7b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-falcon3-7b
    - name: Uninstall Falcon3 7b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-falcon3-7b"
      run_once: true
      tags: uninstall-falcon3-7b
      when:
        - "'falcon3-7b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Mistral_7b LLM Model
      block:
        - name: Delete Ingress resource Mistral 7b from default namespace
          tags: install-mistral-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-mistral-7b-ingress
            state: absent
        - name: Delete Ingress resource Mistral 7b from auth-apisix namespace
          tags: install-mistral-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-mistral-7b-ingress
            state: absent
        - name: Deploy LLM model Mistral_7b
          ansible.builtin.command: >-
            helm upgrade --install vllm-mistral-7b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="mistralai/Mistral-7B-Instruct-v0.3"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}            
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Mistral_7b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "mistralai/Mistral-7B-Instruct-v0.3"
            reg_litellm_model: "openai/mistralai/Mistral-7B-Instruct-v0.3"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-mistral-7b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-mistral-7b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-mistral-7b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-mistral-7b
      when:
        - '"mistral-7b" in model_name_list'
        - install_true == 'true'
    - name: Check if Mistral_7b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-mistral-7b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-mistral-7b
    - name: Uninstall Mistral_7b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-mistral-7b"
      run_once: true
      tags: uninstall-mistral-7b
      when:
        - "'mistral-7b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy tei LLM Model
      block:
        - name: Delete existing Ingress resource tei in auth-apisix namespace
          tags: install-tei
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-tei-ingress
            state: absent
        - name: Delete existing Ingress resource tei in default namespace
          tags: install-tei
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-tei-ingress
            state: absent                
        - name: Deploy LLM model tei
          ansible.builtin.command: >-
            helm upgrade --install vllm-tei "{{ remote_helm_charts_base }}/tei"
            --set EMBEDDING_MODEL_ID="BAAI/bge-base-en-v1.5"            
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}            
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --values "{{ remote_helm_charts_base }}/tei/gaudi-values.yaml"
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            --set teirerank.namespace=default   
            {{ helm_proxy_args | default('') }}         
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register vllm-tei model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "BAAI/bge-base-en-v1.5"
            reg_litellm_model: "openai/BAAI/bge-base-en-v1.5"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-tei-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-tei
            - install-genai-gateway
          run_once: true
          when:
            - "'install-tei' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-tei
      when:
        - "'tei' in model_name_list"
        - install_true == 'true'
    - name: Check if tei Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-tei --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-tei
    - name: Uninstall tei Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-tei"
      run_once: true
      tags: uninstall-tei
      when:        
        - "'tei' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""
   
    - name: Deploy teirerank LLM Model
      block:
        - name: Delete Ingress resource Teirerank from default namespace
          tags: install-rerank
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-teirerank-ingress
            state: absent
        - name: Delete Ingress resource Teirerank from auth-apisix namespace
          tags: install-rerank
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-teirerank-ingress
            state: absent
        - name: Deploy LLM model teirerank
          ansible.builtin.command: >-
            helm upgrade --install vllm-teirerank "{{ remote_helm_charts_base }}/teirerank"
            --set RERANK_MODEL_ID="BAAI/bge-reranker-base"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}            
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --values "{{ remote_helm_charts_base }}/teirerank/gaudi-values.yaml"
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            --set teirerank.namespace=default
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register vllm-teirerank model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "BAAI/bge-reranker-base"
            reg_litellm_model: "openai/BAAI/bge-reranker-base"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-teirerank-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-rerank
            - install-genai-gateway
          run_once: true
          when:
            - "'install-rerank' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-rerank
      when:
        - "'rerank' in model_name_list"        
        - install_true == 'true'
    - name: Check if teirerank Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-teirerank --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-rerank
    - name: Uninstall teirerank Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-teirerank"
      run_once: true
      tags: uninstall-rerank
      when:        
        - "'rerank' in model_name_list"   
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Deepkseek R1 Distill Qwen 32b LLM Model
      block:
        - name: Delete Ingress resource Deepkseek R1 Distill Qwen 32b from default namespace
          tags: install-deepseek-r1-distill-qwen-32b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepkseek-r1-qwen-32b-ingress
            state: absent          
          run_once: true
        - name: Delete Ingress resource Deepkseek R1 Distill Qwen 32b from auth-apisix namespace
          tags: install-deepseek-r1-distill-qwen-32b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepkseek-r1-qwen-32b-ingress
            state: absent
          run_once: true
        - name: Deploy LLM model Deepkseek R1 Distill Qwen 32b
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepkseek-r1-qwen-32b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size=2
              --values {{ gaudi_values_file }}              
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force             
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Deepkseek R1 Distill Qwen 32b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepkseek-r1-qwen-32b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-deepseek-r1-distill-qwen-32b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-deepseek-r1-distill-qwen-32b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true    
      tags: install-deepseek-r1-distill-qwen-32b
      when:                
        - "'deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"        
        - install_true == 'true'

    - name: Check if vllm-deepkseek-r1-qwen-32b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepkseek-r1-qwen-32b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-deepseek-r1-distill-qwen-32b
    - name: Uninstall vllm-deepkseek-r1-qwen-32b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepkseek-r1-qwen-32b"
      run_once: true
      tags: uninstall-deepseek-r1-distill-qwen-32b
      when:
        - "'deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Deepkseek R1 Distill Llama 8b LLM Model
      block:
        - name: Delete Ingress resource Deepkseek R1 Distill Llama 8b from default namespace
          tags: install-deepseek-r1-distill-llama8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepkseek-r1-llama-8b-ingress
            state: absent          
        - name: Delete Ingress resource Deepkseek R1 Distill Llama 8b from auth-apisix namespace
          tags: install-deepseek-r1-distill-llama8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepkseek-r1-llama-8b-ingress
            state: absent          
        - name: Deploy LLM model Deepkseek R1 Distill Llama 8b
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepkseek-r1-llama-8b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
              --values {{ gaudi_values_file }}              
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force           
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Deepkseek R1 Distill Llama 8b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepkseek-r1-llama-8b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-deepseek-r1-distill-llama8b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-deepseek-r1-distill-llama8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-deepseek-r1-distill-llama8b
      when:                        
        - "'deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - install_true == 'true'

    - name: Check if vllm-deepkseek-r1-llama-8b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepkseek-r1-llama-8b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-deepseek-r1-distill-llama8b
    - name: Uninstall vllm-deepkseek-r1-llama-8b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepkseek-r1-llama-8b"
      run_once: true
      tags: uninstall-deepseek-r1-distill-llama8b
      when:
        - "'deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""    

    - name: Deploy Llama3.405b LLM Model
      block:
        - name: Delete Ingress resource Llama3.405b from default namespace
          tags: install-llama3-405b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama3-405b-ingress
            state: absent
        - name: Delete Ingress resource Llama3.405b from auth-apisix namespace
          tags: install-llama3-405b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama3-405b-ingress
            state: absent
        - name: Deploy LLM model Llama3.405b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama3-405b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-405B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size=8
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Llama3.405b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-405B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-405B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama3-405b-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-llama3-405b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama3-405b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags" 
      run_once: true
      tags: install-llama3-405b
      when:
        - "'llama3-405b' in model_name_list"
        - install_true == 'true'

    - name: Check if vllm-llama3-405b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama3-405b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama3-405b
    - name: Uninstall Llama3.405b Model 
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama3-405b"
      run_once: true
      tags: uninstall-llama3-405b
      when:
        - "'llama3-405b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Check if CPU Model is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama8b-cpu --short"
      register: helm_release_installed
      ignore_errors: true      
      run_once: true
      tags: uninstall-cpu-llama-8b
    - name: Uninstall CPU  Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama8b-cpu"
      run_once: true
      tags: uninstall-cpu-llama-8b
      when:
        - model_name_list is defined
        - "'cpu-llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""        
                    
    - name: Deploy CPU based LLM model  llama 8b
      block:        
        - name: Delete Ingress resource Llama8b from default namespace
          tags: install-cpu-llama-8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama8b-cpu-ingress
            state: absent
        - name: Delete Ingress resource Llama8b from auth-apisix namespace
          tags: install-cpu-llama-8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama8b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model  llama 8b Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama8b-cpu "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-8B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force                                                
          register: helm_upgrade_install_model_deployment_cpu_llama8b
          failed_when: helm_upgrade_install_model_deployment_cpu_llama8b.rc != 0          
        - name: Register Xeon/CPU llama 8b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-8B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-8B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama8b-cpu-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002          
          tags:            
            - install-cpu-llama-8b
            - install-genai-gateway
          run_once: true
          when:            
            - "'install-cpu-llama-8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-llama-8b
      when:
        - cpu_playbook == 'true'
        - install_true == 'true'
        - model_name_list is defined
        - "'cpu-llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"          
      
    - name: Check if CPU Deepseek R1 Distill Qwen 32b is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepseek-r1-qwen32b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-qwen-32b
    - name: Uninstall Deepseek R1 Distill Qwen 32b
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepseek-r1-qwen32b-cpu"
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-qwen-32b
      when:
        - model_name_list is defined
        - "'cpu-deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"            
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""        
    
    - name: Deploy CPU Deepseek R1 Distill Qwen 32b LLM model
      block:        
        - name: Delete Ingress resource  Deepseek R1 Distill Qwen 32b from default namespace
          tags: install-cpu-deepseek-r1-distill-qwen-32b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepseek-r1-qwen32b-cpu-ingress
            state: absent
        - name: Delete Ingress resource  Deepseek R1 Distill Qwen 32b from auth-apisix namespace
          tags: install-cpu-deepseek-r1-distill-qwen-32b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepseek-r1-qwen32b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model Deepseek R1 Distill Qwen 32b Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepseek-r1-qwen32b-cpu "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}       
            {{ helm_proxy_args | default('') }}     
            --force                                                
          register: helm_upgrade_install_model_deployment_cpu_deepseek_qwen32b
          failed_when: helm_upgrade_install_model_deployment_cpu_deepseek_qwen32b.rc != 0
        - name: Register Xeon/CPU Deepseek R1 Distill Qwen 32b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepseek-r1-qwen32b-cpu-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-cpu-deepseek-r1-distill-qwen-32b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-deepseek-r1-distill-qwen-32b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-deepseek-r1-distill-qwen-32b
      when:
        - model_name_list is defined
        - cpu_playbook == 'true'
        - install_true == 'true' 
        - "'cpu-deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"

    - name: Check if CPU Deepseek R1 Distill Llama 8B is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepseek-r1-llama8b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-llama8b
    - name: Uninstall Deepseek R1 Distill Llama 8B
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepseek-r1-llama8b-cpu"
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-llama8b
      when:                
        - "'cpu-deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'        
        - helm_release_installed.stdout != ""        
    
    - name: Deploy CPU Deepseek R1 Distill Llama 8B LLM model
      block:        
        - name: Delete Ingress resource Deepseek R1 Distill Llama 8B from default namespace
          tags: install-cpu-deepseek-r1-distill-llama8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepseek-r1-llama8b-cpu-ingress
            state: absent
        - name: Delete Ingress resource Deepseek R1 Distill Llama 8B from auth-apisix namespace
          tags: install-cpu-deepseek-r1-distill-llama8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepseek-r1-llama8b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model Deepseek R1 Distill Llama 8B Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepseek-r1-llama8b-cpu "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}                        
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}            
            {{ helm_proxy_args | default('') }}
            --force                                                
          register: helm_upgrade_install_model_deployment_cpu_deepseek_llama_8b
          failed_when: helm_upgrade_install_model_deployment_cpu_deepseek_llama_8b.rc != 0          
        - name: Register Xeon/CPU Deepseek R1 Distill Llama 8B model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepseek-r1-llama8b-cpu-service/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags: 
            - install-cpu-deepseek-r1-distill-llama8b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-deepseek-r1-distill-llama8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-deepseek-r1-distill-llama8b
      when:
        - cpu_playbook == 'true'
        - install_true == 'true'         
        - "'cpu-deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"
 
    - name: List of Models to be Installed
      tags: always
      run_once: true
      debug:
        var: model_name_list
        
    - name: List the Installed Inference Models
      ansible.builtin.shell:
        cmd: "helm list --short | grep 'vllm-'"
      register: inference_models
      when: list_model_true == 'true'
    - name: Print Installed Models in Comma Separated Format
      ansible.builtin.debug:
        msg: "Installed Models: {{ inference_models.stdout_lines | join(', ') }}"
      when: inference_models.stdout_lines is defined and inference_models.stdout_lines | length > 0
            
    - name: Clean up remote dependencies directory
      tags: always
      ansible.builtin.file:
        path: "{{ remote_helm_charts_base }}"
        state: absent